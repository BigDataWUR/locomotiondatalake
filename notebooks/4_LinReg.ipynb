{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final touch: a decision model with the extracted features \n",
    "\n",
    "We will go very fast through the process of exploratory data analysis, with the data we have available from the gait score experiment.\n",
    "\n",
    "\n",
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load all feature files.\n",
    "\n",
    "First lets read the features from the forceplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you didn't run Notebook 2, get the data from here\n",
    "# !wget https://zenodo.org/record/3563513/files/fp_features.csv\n",
    "fp = spark.read.csv('fp_features.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we read the features from the accelerator sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you didn't run Notebook 3, get the data from here\n",
    "# !wget https://zenodo.org/record/3563513/files/acc_features.csv\n",
    "acc = spark.read.csv('acc_features.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last comes the metadata file, that includes the gait score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = spark.read.csv('files/Walking_trial_IDmatch_edu.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait = metadata.select('Wingband0','Score').withColumnRenamed('Wingband0','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let stitch it alltogether, with two joins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait = gait.join(acc, 'ID')\\\n",
    "           .join(fp, 'ID')\n",
    "\n",
    "gait.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will try to do, is to develop (train and assess) a regression model that will estimate the gait score from the sensor features we have extracted previously.\n",
    "\n",
    "$score = f(steps,timeOnPlate, weight)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(\n",
    "                inputCols=[\"steps\", \"timeOnPlate\", \"weight\"],\n",
    "                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the linear regression learner with default values for the parameters\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.setPredictionCol(\"Predicted_Score\")\\\n",
    "  .setLabelCol(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPipeline = Pipeline()\n",
    "lrPipeline.setStages([vectorizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and inspect a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first train on the trining dataset to see what we get\n",
    "lrModel = lrPipeline.fit(gait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients (i.e., weights) are as follows:\n",
    "weights = lrModel.stages[1].coefficients\n",
    "\n",
    "# The corresponding features for these weights are:\n",
    "featuresNoLabel = vectorizer.getInputCols()\n",
    "\n",
    "\n",
    "# Print coefficients \n",
    "print(list(zip(featuresNoLabel, weights)))\n",
    " \n",
    " # Print the intercept\n",
    "print('intercept', lrModel.stages[1].intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Apply the LR model to the same dataset and predict gait score\n",
    "predictionsLR = lrModel.transform(gait)\\\n",
    "                       .select(\"steps\", \"timeOnPlate\", \"weight\", \"Score\",\"Predicted_Score\")\n",
    "\n",
    " # Print the first rows of the predictions\n",
    "predictionsLR.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! We managed to fit a line between 3 points :)\n",
    "\n",
    "Now let's compute an evaluation metric for our (training) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create an RMSE evaluator using the label and predicted columns \n",
    "regEval = RegressionEvaluator(predictionCol=\"Predicted_Score\", labelCol=\"Score\", metricName=\"rmse\")\n",
    "\n",
    " # Run the evaluator on the DataFrame\n",
    "rmse = regEval.evaluate(predictionsLR)\n",
    "\n",
    "print(\"Root Mean Squared Error: %.20f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-with-Pixiedust_Spark-2.4",
   "language": "python",
   "name": "pythonwithpixiedustspark24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
