{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ETL for the Inertial Measurement Unit(s) sensors\n",
    "\n",
    "An inertial measurement unit (IMU) is an electronic device that measures and \n",
    "reports a body's specific force, angular rate, and sometimes the magnetic field \n",
    "surroundings the body, using a combination of accelerometers and gyroscopes, \n",
    "sometimes also magnetometers.\n",
    "\n",
    "**Yaw**.  \n",
    "The yaw axis (*vertical axis*) has its origin at the center of gravity\n",
    "and is directed towards the bottom of the aircraft, perpendicular to the wings\n",
    "and to the fuselage reference line. Motion about this axis is called yaw. A\n",
    "positive yawing motion moves the nose of the aircraft to the right. The rudder\n",
    "is the primary control of yaw.\n",
    "\n",
    "**Pitch**.  \n",
    "The pitch axis (*transverse* or\n",
    "*lateral axis*) has its origin at the center of gravity and is directed to the\n",
    "right, parallel to a line drawn from wingtip to wingtip. Motion about this axis\n",
    "is called pitch. A positive pitching motion raises the nose of the aircraft and\n",
    "lowers the tail. The elevators are the primary control of pitch.\n",
    "\n",
    "**Roll**.  \n",
    "The\n",
    "roll axis (*longitudinal axis*) has its origin at the center of gravity and is\n",
    "directed forward, parallel to the fuselage reference line. Motion about this\n",
    "axis is called roll. An angular displacement about this axis is called bank. A\n",
    "positive rolling motion lifts the left wing and lowers the right wing. The pilot\n",
    "rolls by increasing the lift on one wing and decreasing it on the other. This\n",
    "changes the bank angle. The ailerons are the primary control of bank. The rudder\n",
    "also has a secondary effect on bank.\n",
    "\n",
    "Below a figure schematically depicting the\n",
    "three aspects; **Roll**, **Yaw**, and **Pitch**.\n",
    "\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Yaw_Axis_Corrected.svg/375px-Yaw_Axis_Corrected.svg.png) \n",
    "\n",
    "\n",
    "In the animal experiment the IMUs were strapped\n",
    "down at three different places, 1) Torso, 2) left leg, and 3) right leg. This\n",
    "has been schematically depicted in the figure below, where the orange squares\n",
    "are the IMUs.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/BigDataWUR/locomotiondatalake/c20ce06eeede72b8af89ce547beee4d3a8182100/images/Picture1_.png\" width=\"250\" height=\"250\" />\n",
    "\n",
    "Thus,\n",
    "now we have an idea of what IMUs are and how they were applied in the\n",
    "experiment. The recorded data from the IMUs were uploaded by us to the datalake\n",
    "stack, note that these were in a native format and need to be pre-processed\n",
    "before we can visualize them and extract features.\n",
    "\n",
    "## First import dependent libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import argparse\n",
    "from subprocess import call\n",
    "import pixiedust\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that Spark works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting native files to readable format \n",
    "\n",
    "Data from the IMUs is in a native format and needs to be pre-processed. For \n",
    "this we now have a customized C++ script (thanks to Jeremy) which takes all \n",
    "available information and puts them in a *txt* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlist = Path(\"files\").glob('**/*.mtb')\n",
    "for filename in pathlist:\n",
    "    subprocess.check_output([\"convertmtb\", filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of the C++ script, all output files are written in the same\n",
    "folder as the original *mtb* file. \n",
    "\n",
    "For more convenience we move the output files\n",
    "to a new folder (here the *accelerometer*-folder as subfolder of *work*). This\n",
    "can be done by the following lines (these are linux commands), where the `!`\n",
    "tells the notebook to exceute this in Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /home/jovyan/files/accelerometer/*.txt /home/jovyan/work/accelerometer/original/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all sensor data\n",
    "\n",
    "Thus, now we have the pre-processed data and can load this in a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall1 = spark.read.csv('work/accelerometer/original/', inferSchema=True, header=True, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link data with turkey id\n",
    "\n",
    "As with the force plate, here we also want to link the filename to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = channelsDFall1.withColumn('input', input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the filename into two parts, one containing the ID the other the\n",
    "IMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('filename', split(df.input,'/')[8])\n",
    "df = df.withColumn('ID_IMUs', split(split(df['input'], '/')[8], \".mtb_\")[0])\n",
    "df = df.withColumn('IMU', split(split(df['input'], '/')[8], \".mtb_\")[1].substr(1,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show again to see if we did want we anticipated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('ID_IMUs','IMU').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.groupBy(df.ID_IMUs).count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in this case the turkey identifier is not part of the file path.\n",
    "Instead, there is a separate metadata file that encodes which IMU have been\n",
    "placed on each turkey.\n",
    "\n",
    "So we are going to load that as well and see what is in\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataDF = spark.read.csv('files/Walking_trial_IDmatch_edu.csv', header=True, sep=\",\")\n",
    "metadataDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Wingband0` column is the turkey identifier, so lets rename it to `ID` for\n",
    "simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataDF = metadataDF.withColumnRenamed('Wingband0', 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to combine the two dataframes based on the IMU identifier, we can do\n",
    "this by the `join` command. \n",
    "\n",
    "<!-- This is too advanced \n",
    "For more information\n",
    "about how to join your data see the figure below or\n",
    "[here](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-\n",
    "apache-spark/). In our case we want to include all data, however we can suffice\n",
    "with an inner join because the metadata file was manually curated. <img\n",
    "src=\"http://kirillpavlov.com/images/join-types.png\" width=\"500\" height=\"500\" />\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(metadataDF, df.ID_IMUs == metadataDF.IMUfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the join was successfull we can show, the columns on which we joined\n",
    "and include the turkey identifier as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('PackedCounter','IMUfiles', 'ID_IMUs', 'ID').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to further automate creating a metadata file, as now it was\n",
    "performed manually by checking the (log) files' start and end time. For the\n",
    "Force Plate the output file has the time of stopping the recording, whereas the\n",
    "IMU (and 3D-video) have the starting time of recording. This information was\n",
    "combined in a matrix and thereafter manually inspected if the time-stamps were\n",
    "rigth for the different files, any discrepancies were written down and filtered\n",
    "out if possible.\n",
    "\n",
    "Now that we have the IMU IDs linked to each turkey, we are going further by \n",
    "processing the data. \n",
    "\n",
    "\n",
    "## Clean data and calculate summary statistics\n",
    "\n",
    "Within the dataframe there is already a column called\n",
    "'StatusWord', which is an indication of whether the device is\n",
    "functioning properly. \n",
    "\n",
    "In the 'StatusWord' column different values are given,\n",
    "where all other values than 2 represent 'flagged' data.\n",
    "\n",
    "First lets calculate some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.ID,df.Roll,df.Yaw,df.Pitch).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be more useful per animal id, as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.ID,df.Roll,df.Yaw,df.Pitch).groupBy('ID').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even per animal and IMU, as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.ID,df.IMU, df.Roll,df.Yaw,df.Pitch).groupBy('ID', 'IMU').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate a single feature\n",
    "\n",
    "Ideally a feature to summarize the accelerometer data, would be to \n",
    "estimate the number of steps that the turkey has taken in the gait walk.\n",
    "\n",
    "As a proxy to this number we will estimate how man times the roll axis \n",
    "measurement changed sign.\n",
    "\n",
    "\n",
    "Lets create a dataframe with the columns we need:\n",
    "\n",
    "- Roll is the rotation on the roll principal axis\n",
    "- ID is the turkey id\n",
    "- IMU is the sensor id\n",
    "- StatusWord is a code about sensor working properly. The flag 2 is used for valid measurements.\n",
    "- PackedCounter is a counter of the packets, i.e. an identifier for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = df.select(df.ID, df.IMU, df.StatusWord, df.PackedCounter, df.Roll)\n",
    "small.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out the erroneous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = small.filter(small.StatusWord==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect sign change on the roll value, we\n",
    "will first add a new column with the value of the previous step \n",
    "(time lag function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = small.withColumn('prev_Roll',\n",
    "                        func.lag(small['Roll'])\n",
    "                                 .over(Window.partitionBy(\"ID\",\"IMU\")\\\n",
    "                                             .orderBy(\"PackedCounter\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we calculate the signum of the current Roll times the Roll of the previous step\n",
    "If it is -1 then the sign changed, if it is +1 then it remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = df_lag.withColumn(\"step\",func.signum(df_lag.Roll*df_lag.prev_Roll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, summarize by counting the minuses per sensor. \n",
    "As we do not know which sensor is attached on which leg, we will use an \n",
    "an estimated feature the minimum signum changes out of the three sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc =  df_lag.filter('step=-1')\\\n",
    "            .groupBy('ID','IMU').count()\\\n",
    "            .groupBy('ID').min()\n",
    "rc.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store in a file\n",
    "\n",
    "As a final step, lets save it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.withColumnRenamed('min(count)','steps').write.csv(\"acc_features.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-with-Pixiedust_Spark-2.4",
   "language": "python",
   "name": "pythonwithpixiedustspark24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
